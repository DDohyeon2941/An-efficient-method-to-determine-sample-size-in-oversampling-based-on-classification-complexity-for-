# Efficient Oversampling Based on Classification Complexity

This repository contains the implementation of an efficient oversampling method designed to handle imbalanced datasets. By leveraging classification complexity measures, this method generates synthetic samples only where they are most needed, reducing oversampling size, improving computational efficiency, and minimizing the risk of overfitting.

## Table of Contents
- [Overview](#overview)
- [Data](#data)
- [Methodology](#methodology)
- [Results](#results)
- [Contributing](#contributing)
- [License](#license)
- [Citation](#citation)

## Overview
Imbalanced datasets are common in many real-world applications and pose significant challenges for machine learning models. Conventional oversampling methods, such as SMOTE, often create excessive minority class samples, which can lead to overfitting. Our method optimizes the number of synthetic samples generated by considering the **classification complexity** of the dataset, ensuring that fewer, but more impactful, samples are generated.

### Key Features:
- **Reduced Oversampling Size**: Unlike traditional methods that generate a balanced number of samples, this approach creates fewer synthetic samples, reducing overfitting and computational costs.
- **Complexity-Aware Sampling**: Incorporates both feature-based and neighborhood-based classification complexity measures to guide the generation of new samples.
- **Boosting Integration**: Works with ensemble techniques like **SMOTEBoost**, **RAMOBoost**, **WOTBoost**, and **K-means SMOTEBoost**, making it suitable for boosting algorithms that are sensitive to noisy or excessive samples.

## Data
The following imbalanced datasets were used to validate this method, sorted by imbalance ratio (IR):

1. **Ionosphere**: 350 samples, 34 attributes, IR = 1.80
2. **KC1**: 1,212 samples, 21 attributes, IR = 2.85
3. **Ecoli**: 336 samples, 7 attributes, IR = 8.60
4. **Optical Digits**: 5,620 samples, 64 attributes, IR = 9.14
5. **Satimage**: 6,435 samples, 36 attributes, IR = 9.28
6. **Abalone**: 4,177 samples, 7 attributes, IR = 9.68
7. **Spectrometer**: 531 samples, 93 attributes, IR = 10.80
8. **Isolet**: 7,797 samples, 617 attributes, IR = 12.00
9. **US Crime**: 1,994 samples, 100 attributes, IR = 12.29
10. **Oil**: 937 samples, 27 attributes, IR = 21.85
11. **Wine Quality**: 3,961 samples, 11 attributes, IR = 21.90
12. **Glass**: 213 samples, 9 attributes, IR = 22.67
13. **Yeast Me2**: 1,453 samples, 8 attributes, IR = 27.49
14. **Letter Img**: 18,668 samples, 16 attributes, IR = 28.17
15. **Mammography**: 7,849 samples, 6 attributes, IR = 29.90
16. **PC1**: 4,901 samples, 6 attributes, IR = 42.76

## Methodology

In this study, four key classification complexity measures are used to determine the efficient oversampling size for imbalanced datasets: **F1**, **F2**, **N1**, and **N2**. These measures provide a detailed understanding of the data's structure and class separability.

### Classification Complexity Measures

#### 1. Feature-Based Complexity Measures
Feature-based measures assess the discriminative power of individual features in distinguishing between classes.

- **F1 (Maximum Fisher's Discriminant Ratio)**:
  - This measure estimates the capability of each feature to separate two classes. It is derived from the **Fisher's Discriminant Ratio (FDR)**, which compares the mean difference between the two classes relative to their variance. The F1 score uses the maximum FDR across all features.
  - Formula:  
    \[
    F1 = \frac{1}{1 + \max_d FDR(f_d)}
    \]
    Where \(FDR(f_d)\) for feature \(d\) is defined as:
    \[
    FDR(f_d) = \frac{(\mu_1 - \mu_2)^2}{\sigma_1^2 + \sigma_2^2}
    \]
    Here, \(\mu_1\), \(\mu_2\) are the means of the two classes, and \(\sigma_1^2\), \(\sigma_2^2\) are their variances.

- **F2 (Class Overlap Measure)**:
  - F2 estimates the overlap between classes along each feature dimension. It is calculated by counting the number of samples that lie in the overlapping region between the two classes for each feature. A higher overlap indicates higher classification complexity.
  - Formula:  
    \[
    F2 = \frac{\min_d \sum_{j=1}^n I(x_{jd} > \max(\min(f^1_d), \min(f^2_d)) \text{ and } x_{jd} < \min(\max(f^1_d), \max(f^2_d)))}{n}
    \]
    where \(f^1_d\) and \(f^2_d\) represent the values of feature \(d\) for the two classes, and \(I(\cdot)\) is the indicator function.

#### 2. Neighborhood-Based Complexity Measures
These measures focus on the local distribution of samples around each instance, particularly the distances to neighbors within and across classes.

- **N1 (Intra/Extra Class Distance Ratio)**:
  - N1 computes the ratio of intra-class distances (distances between a sample and its neighbors from the same class) to extra-class distances (distances to neighbors from the opposite class). A higher ratio indicates that samples are more separable within their own class, reducing classification complexity.
  - Formula:  
    \[
    N1 = \frac{\sum_{i=1}^{n} \sum_{j \in NNk,\text{intra}(x_i)} d(x_i, x_j)}{\sum_{i=1}^{n} \sum_{j \in NNk,\text{extra}(x_i)} d(x_i, x_j) + \sum_{i=1}^{n} \sum_{j \in NNk,\text{intra}(x_i)} d(x_i, x_j)}
    \]

- **N2 (1-Nearest Neighbor Error Rate)**:
  - N2 calculates the error rate of a 1-nearest neighbor (1-NN) classifier using a leave-one-out cross-validation approach. This measures how often a sample is misclassified by its nearest neighbor, providing insight into the local complexity of the class boundaries.
  - Formula:  
    \[
    N2 = \frac{\sum_{i=1}^{n} I(y_i \neq y_{\text{NN1}}(x_i))}{n}
    \]
    where \(y_{\text{NN1}}(x_i)\) is the label of the nearest neighbor of \(x_i\), and \(I(\cdot)\) is the indicator function.

### Summary of Measures:
- **F1** captures the ability of individual features to separate the classes.
- **F2** measures the degree of overlap between the classes along each feature.
- **N1** evaluates the separation between classes based on intra-class and extra-class distances.
- **N2** calculates the error rate of nearest neighbor classification to gauge the local complexity around the class boundary.

These measures are used to adjust the oversampling size dynamically, ensuring that synthetic samples are generated in regions where classification complexity is high, thereby improving performance without excessive oversampling.


## Results
Our experiments across 16 different imbalanced datasets show that the proposed oversampling method achieves superior or comparable performance to traditional oversampling techniques while significantly reducing the number of synthetic samples. This not only reduces computational time but also mitigates the risk of overfitting caused by excessive sampling.

### Key Benefits:
- **Efficiency**: Reduces computational costs by generating fewer synthetic samples.
- **Generalization**: Improves model performance by focusing synthetic sample generation on areas where classification is most difficult, avoiding overfitting.
- **Robustness**: Particularly effective when combined with ensemble learning methods, such as boosting algorithms, where excessive minority samples can lead to overfitting.

## Contributing

We welcome contributions to this project. Please submit pull requests or open issues for any bugs or enhancements.

## References
For more details, please refer to the full paper on [ELSEVIER](https://doi.org/10.1016/j.eswa.2021.115442)
