# Efficient Oversampling Based on Classification Complexity

This repository contains the implementation of an efficient oversampling method designed to handle imbalanced datasets. By leveraging classification complexity measures, this method generates synthetic samples only where they are most needed, reducing oversampling size, improving computational efficiency, and minimizing the risk of overfitting.

## Table of Contents
- [Overview](#overview)
- [Data](#data)
- [Methodology](#methodology)
- [Results](#results)
- [Contributing](#contributing)
- [License](#license)
- [Citation](#citation)

## Overview
Imbalanced datasets are common in many real-world applications and pose significant challenges for machine learning models. Conventional oversampling methods, such as SMOTE, often create excessive minority class samples, which can lead to overfitting. Our method optimizes the number of synthetic samples generated by considering the **classification complexity** of the dataset, ensuring that fewer, but more impactful, samples are generated.

### Key Features:
- **Reduced Oversampling Size**: Unlike traditional methods that generate a balanced number of samples, this approach creates fewer synthetic samples, reducing overfitting and computational costs.
- **Complexity-Aware Sampling**: Incorporates both feature-based and neighborhood-based classification complexity measures to guide the generation of new samples.
- **Boosting Integration**: Works with ensemble techniques like **SMOTEBoost**, **RAMOBoost**, **WOTBoost**, and **K-means SMOTEBoost**, making it suitable for boosting algorithms that are sensitive to noisy or excessive samples.

## Data
The following imbalanced datasets were used to validate this method, sorted by imbalance ratio (IR):

1. **Ionosphere**: 350 samples, 34 attributes, IR = 1.80
2. **KC1**: 1,212 samples, 21 attributes, IR = 2.85
3. **Ecoli**: 336 samples, 7 attributes, IR = 8.60
4. **Optical Digits**: 5,620 samples, 64 attributes, IR = 9.14
5. **Satimage**: 6,435 samples, 36 attributes, IR = 9.28
6. **Abalone**: 4,177 samples, 7 attributes, IR = 9.68
7. **Spectrometer**: 531 samples, 93 attributes, IR = 10.80
8. **Isolet**: 7,797 samples, 617 attributes, IR = 12.00
9. **US Crime**: 1,994 samples, 100 attributes, IR = 12.29
10. **Oil**: 937 samples, 27 attributes, IR = 21.85
11. **Wine Quality**: 3,961 samples, 11 attributes, IR = 21.90
12. **Glass**: 213 samples, 9 attributes, IR = 22.67
13. **Yeast Me2**: 1,453 samples, 8 attributes, IR = 27.49
14. **Letter Img**: 18,668 samples, 16 attributes, IR = 28.17
15. **Mammography**: 7,849 samples, 6 attributes, IR = 29.90
16. **PC1**: 4,901 samples, 6 attributes, IR = 42.76


## Methodology

### Classification Complexity Measures

#### 1. **Feature-Based Complexity Measures**

Feature-based measures assess the discriminative power of individual features in distinguishing between classes.

- **F1 (Maximum Fisher's Discriminant Ratio)**:

  **Original Definition**:  
  This measure estimates the capability of each feature to separate two classes. It is derived from the **Fisher's Discriminant Ratio (FDR)**, which compares the mean difference between the two classes relative to their variance. The F1 score uses the maximum FDR across all features.
  
  Formula:  
  $
  F1 = \frac{1}{1 + \max_d FDR(f_d)}
  $
  where \( FDR(f_d) \) for feature \( d \) is defined as:
  $$
  FDR(f_d) = \frac{(\mu_1 - \mu_2)^2}{\sigma_1^2 + \sigma_2^2}
  $$
  Here, \( \mu_1 \) and \( \mu_2 \) are the means of the two classes, and \( \sigma_1^2 \) and \( \sigma_2^2 \) are their variances.

  **Modification**:  
  The F1 measure now specifically focuses on features where the separability between the minority and majority classes is lower. This helps to ensure that attention is given to regions of the feature space where the minority class struggles the most, guiding synthetic sample generation where it’s most needed.

- **F2 (Class Overlap Measure)**:

  **Original Definition**:  
  F2 estimates the overlap between classes along each feature dimension. It is calculated by counting the number of samples that lie in the overlapping region between the two classes for each feature. A higher overlap indicates higher classification complexity.
  
  Formula:  
  $$
  F2 = \frac{\min_d \sum_{j=1}^n I(x_{jd} > \max(\min(f^1_d), \min(f^2_d)) \text{ and } x_{jd} < \min(\max(f^1_d), \max(f^2_d)))}{n}
  $$
  where \( f^1_d \) and \( f^2_d \) represent the values of feature \( d \) for the two classes, and \( I(\cdot) \) is the indicator function.

  **Modification**:  
  The F2 measure is adapted to include **boundary samples** in the calculation. This modification captures the complexity at the decision boundary, which is particularly critical for imbalanced datasets where the minority class tends to lie near the majority class in these regions. Including these boundary samples helps in identifying areas that need synthetic samples to improve classification performance.

---

#### 2. **Neighborhood-Based Complexity Measures**

Neighborhood-based measures focus on the local distribution of samples around each instance, particularly the distances to neighbors within and across classes.

- **N1 (Intra/Extra Class Distance Ratio)**:

  **Original Definition**:  
  N1 computes the ratio of intra-class distances (distances between a sample and its neighbors from the same class) to extra-class distances (distances to neighbors from the opposite class). A higher ratio indicates that samples are more separable within their own class, reducing classification complexity.
  
  Formula:  
  $$
  N1 = \frac{\sum_{i=1}^{n} \sum_{j \in \text{NNk, intra}(x_i)} d(x_i, x_j)}{\sum_{i=1}^{n} \sum_{j \in \text{NNk, extra}(x_i)} d(x_i, x_j) + \sum_{i=1}^{n} \sum_{j \in \text{NNk, intra}(x_i)} d(x_i, x_j)}
  $$

  **Modification**:  
  The N1 measure is extended to consider the **k-nearest neighbors (kNN)** instead of only the nearest neighbor. Additionally, this measure now focuses specifically on the minority class samples, ensuring that the complexity of separating minority samples from majority samples is captured more effectively, which is crucial in imbalanced datasets.

- **N2 (1-Nearest Neighbor Error Rate)**:

  **Original Definition**:  
  N2 calculates the error rate of a 1-nearest neighbor (1-NN) classifier using a leave-one-out cross-validation approach. This measures how often a sample is misclassified by its nearest neighbor, providing insight into the local complexity of the class boundaries.
  
  Formula:  
  $$
  N2 = \frac{\sum_{i=1}^{n} I(y_i \neq y_{\text{NN1}}(x_i))}{n}
  $$
  where \( y_{\text{NN1}}(x_i) \) is the label of the nearest neighbor of \( x_i \), and \( I(\cdot) \) is the indicator function.

  **Modification**:  
  N2 is adapted to use **k-nearest neighbors (kNN)** instead of only one nearest neighbor, providing a more comprehensive assessment of local complexity. Additionally, this measure is now focused on minority class samples, capturing the specific challenges faced by minority samples in their nearest-neighbor relationships.

---

### Summary of Modifications:

- **F1** now focuses on features with lower separability for the minority class.
- **F2** includes boundary samples to better capture class overlap in critical decision boundary regions.
- **N1** is modified to use k-nearest neighbors and focuses specifically on minority class samples to reflect the difficulty of separating minority instances from the majority.
- **N2** is extended to use k-nearest neighbors and focuses on the error rate for minority class samples, offering a more nuanced view of complexity in imbalanced datasets.

These modifications ensure that the complexity measures are better suited to address the specific challenges presented by imbalanced datasets, guiding synthetic sampling to the regions where it’s most needed.

## Results
Our experiments across 16 different imbalanced datasets show that the proposed oversampling method achieves superior or comparable performance to traditional oversampling techniques while significantly reducing the number of synthetic samples. This not only reduces computational time but also mitigates the risk of overfitting caused by excessive sampling.

### Key Benefits:
- **Efficiency**: Reduces computational costs by generating fewer synthetic samples.
- **Generalization**: Improves model performance by focusing synthetic sample generation on areas where classification is most difficult, avoiding overfitting.
- **Robustness**: Particularly effective when combined with ensemble learning methods, such as boosting algorithms, where excessive minority samples can lead to overfitting.

## Contributing

We welcome contributions to this project. Please submit pull requests or open issues for any bugs or enhancements.

## References
For more details, please refer to the full paper on [ELSEVIER](https://doi.org/10.1016/j.eswa.2021.115442)
