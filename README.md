# Efficient Oversampling Based on Classification Complexity

This repository contains the implementation of an efficient oversampling method designed to handle imbalanced datasets. By leveraging classification complexity measures, this method generates synthetic samples only where they are most needed, reducing oversampling size, improving computational efficiency, and minimizing the risk of overfitting.

## Table of Contents
- [Overview](#overview)
- [Data](#data)
- [Methodology](#methodology)
- [Results](#results)
- [Contributing](#contributing)
- [License](#license)
- [Citation](#citation)

## Overview
Imbalanced datasets are common in many real-world applications and pose significant challenges for machine learning models. Conventional oversampling methods, such as SMOTE, often create excessive minority class samples, which can lead to overfitting. Our method optimizes the number of synthetic samples generated by considering the **classification complexity** of the dataset, ensuring that fewer, but more impactful, samples are generated.

### Key Features:
- **Reduced Oversampling Size**: Unlike traditional methods that generate a balanced number of samples, this approach creates fewer synthetic samples, reducing overfitting and computational costs.
- **Complexity-Aware Sampling**: Incorporates both feature-based and neighborhood-based classification complexity measures to guide the generation of new samples.
- **Boosting Integration**: Works with ensemble techniques like **SMOTEBoost**, **RAMOBoost**, **WOTBoost**, and **K-means SMOTEBoost**, making it suitable for boosting algorithms that are sensitive to noisy or excessive samples.

## Data
The following imbalanced datasets were used to validate this method, sorted by imbalance ratio (IR):

1. **Ionosphere**: 350 samples, 34 attributes, IR = 1.80
2. **KC1**: 1,212 samples, 21 attributes, IR = 2.85
3. **Ecoli**: 336 samples, 7 attributes, IR = 8.60
4. **Optical Digits**: 5,620 samples, 64 attributes, IR = 9.14
5. **Satimage**: 6,435 samples, 36 attributes, IR = 9.28
6. **Abalone**: 4,177 samples, 7 attributes, IR = 9.68
7. **Spectrometer**: 531 samples, 93 attributes, IR = 10.80
8. **Isolet**: 7,797 samples, 617 attributes, IR = 12.00
9. **US Crime**: 1,994 samples, 100 attributes, IR = 12.29
10. **Oil**: 937 samples, 27 attributes, IR = 21.85
11. **Wine Quality**: 3,961 samples, 11 attributes, IR = 21.90
12. **Glass**: 213 samples, 9 attributes, IR = 22.67
13. **Yeast Me2**: 1,453 samples, 8 attributes, IR = 27.49
14. **Letter Img**: 18,668 samples, 16 attributes, IR = 28.17
15. **Mammography**: 7,849 samples, 6 attributes, IR = 29.90
16. **PC1**: 4,901 samples, 6 attributes, IR = 42.76

## Methodology
### Classification Complexity Measures:
1. **Feature-based complexity**: Measures the discriminatory power of features to determine how separable the classes are. This helps identify whether more synthetic samples are necessary in specific areas of the feature space.
2. **Neighborhood-based complexity**: Utilizes nearest-neighbor distances to evaluate the density and separability of classes at the instance level. This ensures that samples are generated where classification is most challenging.

## Results
Our experiments across 16 different imbalanced datasets show that the proposed oversampling method achieves superior or comparable performance to traditional oversampling techniques while significantly reducing the number of synthetic samples. This not only reduces computational time but also mitigates the risk of overfitting caused by excessive sampling.

### Key Benefits:
- **Efficiency**: Reduces computational costs by generating fewer synthetic samples.
- **Generalization**: Improves model performance by focusing synthetic sample generation on areas where classification is most difficult, avoiding overfitting.
- **Robustness**: Particularly effective when combined with ensemble learning methods, such as boosting algorithms, where excessive minority samples can lead to overfitting.

## Contributing

We welcome contributions to this project. Please submit pull requests or open issues for any bugs or enhancements.

## References
For more details, please refer to the full paper on [https://doi.org/10.1016/j.eswa.2021.115442]
